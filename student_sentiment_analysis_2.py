# -*- coding: utf-8 -*-
"""student_sentiment_analysis_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12vUW6UATCp7jWsr_LsTA60X8maBrFium
"""

# General packages
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

# NLP packages
import nltk
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud

# Modeling packages
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

from pylab import rcParams
import warnings
warnings.filterwarnings("ignore")
rcParams['figure.figsize'] = 14, 6
plt.style.use('ggplot')

data = pd.read_csv("Data_student.csv")

data.head(10)

## Getting the number of words by splitting them by a space
words_per_review = data.Answer.apply(lambda x: len(x.split(" ")))
words_per_review.hist(bins = 100)
plt.xlabel('Review Length (words)')
plt.ylabel('Frequency')
plt.show()

print('Average words:', words_per_review.mean())
print('Skewness:', words_per_review.skew())

# Let's also look at the distribution of ratings:
percent_val = 100 * data['Answer'].value_counts()/len(data)
percent_val

# percent_val.plot.bar()
# plt.show()

# Text visualization using word clouds (word clouds )Â¶

word_cloud_text = ''.join(data['Answer'])

wordcloud = WordCloud(max_font_size=100, # Maximum font size for the largest word
                      max_words=100, # The maximum number of words
                      background_color="white", # Background color for the word cloud image
                      scale = 10, # Scaling between computation and drawing
                      width=800, # Width of the canvas
                      height=400 # Height of the canvas
                     ).generate(word_cloud_text)

plt.figure()
plt.imshow(wordcloud, 
           interpolation="bilinear") # to make the displayed image appear more smoothly
plt.axis("off")
plt.show()

"""Pre-processing"""

# 1. Converting words to lower/upper case

data['answer_text_new'] = data['Answer'].str.lower()

# Word Tokenisation

from nltk import word_tokenize
import nltk
nltk.download('punkt')

# For reviews not converted to lowe case
token_lists = [word_tokenize(each) for each in data['answer_text_new']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens then: ",len(set(tokens)))

# For reviews converted to lowe case
token_lists_lower = [word_tokenize(each) for each in data['answer_text_new']]
tokens_lower = [item for sublist in token_lists_lower for item in sublist]
print("Number of unique tokens now: ",len(set(tokens_lower)))

# 2. Removing special characters

### Selecting non alpha numeric charactes that are not spaces
spl_chars = data['answer_text_new'].apply(lambda review: 
                                                     [char for char in list(review) if not char.isalnum() and char != ' '])

## Getting list of list into a single list
flat_list = [item for sublist in spl_chars for item in sublist]

## Unique special characters
set(flat_list)

# Remove the above special characters

review_backup = data['answer_text_new'].copy()
data['answer_text_new'] = data['answer_text_new'].str.replace(r'[^A-Za-z0-9 ]+', ' ')

print("- Old Review -")
print(review_backup.values[6])
print("\n- New Review -")
print(data['answer_text_new'][6])

token_lists = [word_tokenize(each) for each in data['Answer']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens then: ",len(set(tokens)))

token_lists = [word_tokenize(each) for each in data['answer_text_new']]
tokens = [item for sublist in token_lists for item in sublist]
print("Number of unique tokens now: ",len(set(tokens)))

"""3. Stopwords and high/low frequency words"""

from nltk.corpus import stopwords
nltk.download('stopwords')

print('Available languages for NLTK v.3.4.5: ')
print(stopwords.fileids())

# Let's now review the list of English stopwords:

noise_words = []
eng_stop_words = stopwords.words('english')
eng_stop_words

stop_words = set(eng_stop_words)
without_stop_words = []
stopword = []
sentence = data['answer_text_new'][0]
words = nltk.word_tokenize(sentence)

for word in words:
    if word in stop_words:
        stopword.append(word)
    else:
        without_stop_words.append(word)

print('-- Original Sentence --\n', sentence)
print('\n-- Stopwords in the sentence --\n', stopword)
print('\n-- Non-stopwords in the sentence --\n', without_stop_words)

def stopwords_removal(stop_words, sentence):
    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]

data['answer_text_nonstop'] = data['answer_text_new'].apply(lambda row: stopwords_removal(stop_words, row))
data[['answer_text_new','answer_text_nonstop']]

"""4. Stemming & lemmatization"""

from nltk.stem import PorterStemmer, LancasterStemmer # Common stemmers
from nltk.stem import WordNetLemmatizer # Common Lematizer
nltk.download('wordnet')
from nltk.corpus import wordnet

porter = PorterStemmer()
lancaster = LancasterStemmer()
lemmatizer = WordNetLemmatizer()

print("Lancaster Stemmer")
print(lancaster.stem("trouble"))
print(lancaster.stem("troubling"))
print(lancaster.stem("troubled"))

# Provide a word to be lemmatized
print("WordNet Lemmatizer")
print(lemmatizer.lemmatize("trouble", wordnet.NOUN))
print(lemmatizer.lemmatize("troubling", wordnet.VERB))
print(lemmatizer.lemmatize("troubled", wordnet.VERB))

data.head()

data.columns

# rating = {'Positive': 1,'Negative': 0,'Neutral': 0.5,'Ambiguous': '-'}

rating = {'Positive': 1,'Negative': 0,'Neutral': 0.5,'Ambiguous': 2}
  
data['rating'] = data['Sentiment (Positive / Negative / Neutral / Ambiguous)'].map(rating)
data.head()



data.shape

data_back = data

data = data[data['rating'] != 0.5]
data.shape

data = data[data['rating'] != 2]
data.shape

data = data.dropna()

data1.isnull().any()

"""Building a machine learning model"""

data[['answer_text_nonstop','rating']].head(20)
data_back = data

data = data_back[['answer_text_new','rating']]

data.head()

# Bag of words

from nltk import ngrams

# The following code creates a word-document matrix.
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(data['answer_text_new'])
df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())
df.head()

### Creating a python object of the class CountVectorizer

bow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization
                             stop_words=noise_words, # List of stopwords
                             ngram_range=(1,1)) # number of n-grams

bow_data = bow_counts.fit_transform(data['answer_text_new'])

bow_data

X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features
                                                                    data['rating'], # Target variable
                                                                    test_size = 0.2, # 20% test size
                                                                    random_state = 0) # random state for replication purposes

y_test_bow.value_counts()/y_test_bow.shape[0]

# len(data[data['rating'] == '-'])
len(data[data['rating'] == 2])

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)

X_train_bow.shape

y_train_bow.shape

# np.isnan(X_train_bow).any(), np.isnan(y_train_bow).any(), np.isnan(X_test_bow).any()
# data.isnull().any()

# data1 = data.dropna()

type(X_train_bow)

type(y_train_bow)

data.isnull().any()

"""Applying logistic regression"""

### Training the model 
lr_model_all = LogisticRegression() # Logistic regression
lr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model

## Predicting the output
test_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction

## Calculate key performance metrics
print("F1 score: ", f1_score(y_test_bow, test_pred_lr_all))

### Changes with respect to the previous code
### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)
### 2. Including the stopwords in the bag of words features

bow_counts = CountVectorizer(tokenizer= word_tokenize,
                             ngram_range=(1,4))

bow_data = bow_counts.fit_transform(data.answer_text_new)

# Notice the increase in features with inclusion of n-grams
bow_data

X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,
                                                                    data['rating'],
                                                                    test_size = 0.2,
                                                                    random_state = 0)

# Defining and training the model
lr_model_all_new = LogisticRegression(max_iter = 200)
lr_model_all_new.fit(X_train_bow, y_train_bow)

# Predicting the results
test_pred_lr_all = lr_model_all_new.predict(X_test_bow)

print("F1 score: ", f1_score(y_test_bow,test_pred_lr_all))

lr_weights = pd.DataFrame(list(zip(bow_counts.get_feature_names(), # ge tall the n-gram feature names
                                   lr_model_all_new.coef_[0])), # get the logistic regression coefficients
                          columns= ['words','weights']) # defining the colunm names

lr_weights.sort_values(['weights'], ascending = False)[:15] # top-15 more important features for positive reviews

lr_weights.sort_values(['weights'], ascending = False)[-15:] # top-15 more important features for negative reviews

"""**TF-IDF model**"""

from sklearn.feature_extraction.text import TfidfVectorizer

### Creating a python object of the class CountVectorizer
tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization
                               stop_words=noise_words, # List of stopwords
                               ngram_range=(1,1)) # number of n-grams

tfidf_data = tfidf_counts.fit_transform(data['answer_text_new'])

tfidf_data

X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,
                                                                            data['rating'],
                                                                            test_size = 0.2,
                                                                            random_state = 0)

"""Applying logistic regression to TF-IDF features"""

### Setting up the model class
lr_model_tf_idf = LogisticRegression()

## Training the model 
lr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)

## Prediciting the results
test_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)

## Evaluating the model
print("F1 score: ",f1_score(y_test_bow, test_pred_lr_all))